{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "20I3bqY9Tnq-"
      },
      "source": [
        "#imports \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from collections import Counter\n",
        " \n",
        "#sklearn\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "\n",
        "\n",
        "#tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "0bYeDWNnbtRv",
        "outputId": "e45ca92f-e2e3-4555-ec04-602457f1643c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zznMOkZMTnrD"
      },
      "source": [
        "## Preparing data for ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLboX9JwTnrE"
      },
      "source": [
        "#loading the dataset we created \n",
        "df = pd.read_csv(\"covid_dataset.csv\")\n",
        " \n",
        "#remove filename column \n",
        "df.drop([\"filename\"], axis=1, inplace=True)\n",
        "df = df[df['label'] != 'unknown']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJKurk02TnrE"
      },
      "source": [
        "#mapping for the results \n",
        "map_dict = {\"negative\":0, \"positive\":1}\n",
        " \n",
        "#mapping the values of the dict \n",
        "df['label'] = df['label'].map(map_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTFA_n30TnrH"
      },
      "source": [
        "#labels\n",
        "y = df['label'].to_numpy()\n",
        " \n",
        "#rest of data\n",
        "X = (df.iloc[:, :-1]).to_numpy()\n",
        "\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "#cc = ClusterCentroids(random_state=0)\n",
        "#X_resampled, y_resampled = cc.fit_resample(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzagubWiTnrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "887ec2cd-5132-4844-b593-e8f8b07f7910"
      },
      "source": [
        "#instantiate the Label Encoder \n",
        "le = LabelEncoder()\n",
        " \n",
        "#fit and transform the encoder \n",
        "y_resampled = le.fit_transform(y_resampled)\n",
        " \n",
        "#instantiate scaler \n",
        "stsc = StandardScaler()\n",
        " \n",
        "#fit transform scaler \n",
        "X_resampled = stsc.fit_transform(X_resampled)\n",
        " \n",
        "#separating the data \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, train_size=0.75)\n",
        "\n",
        "#initialize callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "#class weights\n",
        "class_weight = {0: 1.,\n",
        "                1: 3.}\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2926, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ5FXVfkzqjj"
      },
      "source": [
        "## Building an ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-pWwJInp_VC",
        "outputId": "e5cb504a-b18e-4f55-e49a-1626ae60e146"
      },
      "source": [
        "inputs = np.concatenate((X_train, X_test), axis=0)\n",
        "targets = np.concatenate((y_train, y_test), axis=0)\n",
        "acc_per_fold=[]\n",
        "loss_per_fold=[]\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=10, shuffle=True,random_state=42)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "\n",
        "\n",
        "#initialize a sequential model \n",
        "for train, test in kfold.split(inputs, targets):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(2, activation='softmax'))\n",
        "\n",
        "  # Compile the model\n",
        "    model.compile(optimizer='adam', \n",
        "              loss = 'sparse_categorical_crossentropy', \n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "    print('-----------------------')\n",
        "\n",
        "    print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "    history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=128,\n",
        "              epochs=50,\n",
        "              validation_data=(X_test, y_test),\n",
        "              callbacks=[early_stopping, mc]\n",
        "              )\n",
        "\n",
        "  # Generate generalization metrics\n",
        "    scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    \n",
        "    acc_per_fold.append(scores[1] * 100)\n",
        "    loss_per_fold.append(scores[0])\n",
        "\n",
        "  # Increase fold number\n",
        "    fold_no = fold_no + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 10ms/step - loss: 0.6911 - accuracy: 0.5533 - val_loss: 0.6905 - val_accuracy: 0.5082\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50820, saving model to best_model.h5\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5303 - val_loss: 0.6677 - val_accuracy: 0.5768\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.50820 to 0.57684, saving model to best_model.h5\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6691 - accuracy: 0.5832 - val_loss: 0.6563 - val_accuracy: 0.5891\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.57684 to 0.58914, saving model to best_model.h5\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6591 - accuracy: 0.6081 - val_loss: 0.6455 - val_accuracy: 0.6209\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.58914 to 0.62090, saving model to best_model.h5\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6434 - accuracy: 0.6221 - val_loss: 0.6424 - val_accuracy: 0.6291\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.62090 to 0.62910, saving model to best_model.h5\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6324 - accuracy: 0.6506 - val_loss: 0.6201 - val_accuracy: 0.6588\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.62910 to 0.65881, saving model to best_model.h5\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6203 - accuracy: 0.6511 - val_loss: 0.5995 - val_accuracy: 0.6998\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.65881 to 0.69980, saving model to best_model.h5\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5923 - accuracy: 0.6870 - val_loss: 0.5707 - val_accuracy: 0.7008\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.69980 to 0.70082, saving model to best_model.h5\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5725 - accuracy: 0.6974 - val_loss: 0.5563 - val_accuracy: 0.7111\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.70082 to 0.71107, saving model to best_model.h5\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5456 - accuracy: 0.7099 - val_loss: 0.5371 - val_accuracy: 0.7367\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.71107 to 0.73668, saving model to best_model.h5\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5184 - accuracy: 0.7516 - val_loss: 0.4976 - val_accuracy: 0.7582\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.73668 to 0.75820, saving model to best_model.h5\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4921 - accuracy: 0.7664 - val_loss: 0.5180 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.75820\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4756 - accuracy: 0.7682 - val_loss: 0.4781 - val_accuracy: 0.7613\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.75820 to 0.76127, saving model to best_model.h5\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4402 - accuracy: 0.7879 - val_loss: 0.4209 - val_accuracy: 0.8176\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.76127 to 0.81762, saving model to best_model.h5\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3912 - accuracy: 0.8312 - val_loss: 0.4076 - val_accuracy: 0.8166\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.81762\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3844 - accuracy: 0.8281 - val_loss: 0.3811 - val_accuracy: 0.8320\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.81762 to 0.83197, saving model to best_model.h5\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 0.3327 - accuracy: 0.8750 - val_loss: 0.3475 - val_accuracy: 0.8658\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.83197 to 0.86578, saving model to best_model.h5\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3080 - accuracy: 0.8724 - val_loss: 0.3400 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.86578\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2837 - accuracy: 0.8901 - val_loss: 0.3398 - val_accuracy: 0.8576\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.86578\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2678 - accuracy: 0.8876 - val_loss: 0.2945 - val_accuracy: 0.8873\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.86578 to 0.88730, saving model to best_model.h5\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2249 - accuracy: 0.9163 - val_loss: 0.3051 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.88730\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2062 - accuracy: 0.9273 - val_loss: 0.2747 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.88730 to 0.90369, saving model to best_model.h5\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.9351 - val_loss: 0.2594 - val_accuracy: 0.9139\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.90369 to 0.91393, saving model to best_model.h5\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1661 - accuracy: 0.9346 - val_loss: 0.2671 - val_accuracy: 0.9221\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.91393 to 0.92213, saving model to best_model.h5\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1595 - accuracy: 0.9441 - val_loss: 0.2494 - val_accuracy: 0.9293\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.92213 to 0.92930, saving model to best_model.h5\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1283 - accuracy: 0.9608 - val_loss: 0.2287 - val_accuracy: 0.9314\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.92930 to 0.93135, saving model to best_model.h5\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1293 - accuracy: 0.9574 - val_loss: 0.2885 - val_accuracy: 0.9098\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.93135\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1179 - accuracy: 0.9640 - val_loss: 0.2906 - val_accuracy: 0.9191\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93135\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1206 - accuracy: 0.9568 - val_loss: 0.3046 - val_accuracy: 0.9016\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.93135\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1388 - accuracy: 0.9510 - val_loss: 0.2899 - val_accuracy: 0.9232\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.93135\n",
            "Epoch 31/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1400 - accuracy: 0.9516 - val_loss: 0.2551 - val_accuracy: 0.9262\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.93135\n",
            "Epoch 00031: early stopping\n",
            "Score for fold 1: loss of 1.8731220960617065; accuracy of 48.33759665489197%\n",
            "-----------------------\n",
            "Training for fold 2 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 8ms/step - loss: 0.6927 - accuracy: 0.5091 - val_loss: 0.6830 - val_accuracy: 0.5625\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.93135\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6814 - accuracy: 0.5755 - val_loss: 0.6738 - val_accuracy: 0.5707\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.93135\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6702 - accuracy: 0.5915 - val_loss: 0.6596 - val_accuracy: 0.6014\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.93135\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6556 - accuracy: 0.6068 - val_loss: 0.6491 - val_accuracy: 0.6270\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93135\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6421 - accuracy: 0.6422 - val_loss: 0.6346 - val_accuracy: 0.6373\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.93135\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6252 - accuracy: 0.6542 - val_loss: 0.6169 - val_accuracy: 0.6670\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93135\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6132 - accuracy: 0.6568 - val_loss: 0.6005 - val_accuracy: 0.6721\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93135\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.6956 - val_loss: 0.5783 - val_accuracy: 0.6936\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93135\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5675 - accuracy: 0.7095 - val_loss: 0.5722 - val_accuracy: 0.6865\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.93135\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5461 - accuracy: 0.7184 - val_loss: 0.5290 - val_accuracy: 0.7316\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93135\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.5137 - accuracy: 0.7468 - val_loss: 0.4995 - val_accuracy: 0.7684\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93135\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4779 - accuracy: 0.7886 - val_loss: 0.5031 - val_accuracy: 0.7357\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93135\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4563 - accuracy: 0.7877 - val_loss: 0.4925 - val_accuracy: 0.7469\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.93135\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4234 - accuracy: 0.8058 - val_loss: 0.4635 - val_accuracy: 0.7756\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.93135\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4188 - accuracy: 0.7983 - val_loss: 0.4105 - val_accuracy: 0.8156\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.93135\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.8412 - val_loss: 0.3751 - val_accuracy: 0.8330\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.93135\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3317 - accuracy: 0.8601 - val_loss: 0.3767 - val_accuracy: 0.8412\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.93135\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3174 - accuracy: 0.8592 - val_loss: 0.3649 - val_accuracy: 0.8248\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.93135\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2872 - accuracy: 0.8826 - val_loss: 0.3045 - val_accuracy: 0.8678\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.93135\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2762 - accuracy: 0.8790 - val_loss: 0.3046 - val_accuracy: 0.8770\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.93135\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2507 - accuracy: 0.8968 - val_loss: 0.2791 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.93135\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2176 - accuracy: 0.9112 - val_loss: 0.2591 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.93135\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1878 - accuracy: 0.9347 - val_loss: 0.2509 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.93135\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1741 - accuracy: 0.9374 - val_loss: 0.2467 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.93135\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1687 - accuracy: 0.9309 - val_loss: 0.2359 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.93135\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1551 - accuracy: 0.9454 - val_loss: 0.2448 - val_accuracy: 0.9068\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.93135\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1467 - accuracy: 0.9488 - val_loss: 0.2527 - val_accuracy: 0.9119\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.93135\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1268 - accuracy: 0.9553 - val_loss: 0.2623 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93135\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1439 - accuracy: 0.9425 - val_loss: 0.2440 - val_accuracy: 0.9119\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.93135\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1517 - accuracy: 0.9432 - val_loss: 0.2391 - val_accuracy: 0.9139\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.93135\n",
            "Epoch 00030: early stopping\n",
            "Score for fold 2: loss of 1.7169853448867798; accuracy of 49.87212419509888%\n",
            "-----------------------\n",
            "Training for fold 3 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 8ms/step - loss: 0.6952 - accuracy: 0.5181 - val_loss: 0.6866 - val_accuracy: 0.5574\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.93135\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6838 - accuracy: 0.5752 - val_loss: 0.6781 - val_accuracy: 0.5594\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.93135\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6768 - accuracy: 0.5641 - val_loss: 0.6675 - val_accuracy: 0.5881\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.93135\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6614 - accuracy: 0.6001 - val_loss: 0.6610 - val_accuracy: 0.5994\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93135\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6579 - accuracy: 0.6036 - val_loss: 0.6559 - val_accuracy: 0.6055\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.93135\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6368 - accuracy: 0.6361 - val_loss: 0.6316 - val_accuracy: 0.6414\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93135\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6194 - accuracy: 0.6591 - val_loss: 0.6166 - val_accuracy: 0.6537\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93135\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6120 - accuracy: 0.6678 - val_loss: 0.5874 - val_accuracy: 0.6834\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93135\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.5828 - accuracy: 0.7060 - val_loss: 0.5763 - val_accuracy: 0.7059\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.93135\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5465 - accuracy: 0.7224 - val_loss: 0.5883 - val_accuracy: 0.6926\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93135\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5490 - accuracy: 0.7201 - val_loss: 0.5248 - val_accuracy: 0.7469\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93135\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.7556 - val_loss: 0.5101 - val_accuracy: 0.7674\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93135\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4879 - accuracy: 0.7738 - val_loss: 0.4915 - val_accuracy: 0.7715\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.93135\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4467 - accuracy: 0.8016 - val_loss: 0.4649 - val_accuracy: 0.7920\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.93135\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4195 - accuracy: 0.8190 - val_loss: 0.4435 - val_accuracy: 0.7941\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.93135\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3960 - accuracy: 0.8213 - val_loss: 0.4297 - val_accuracy: 0.8105\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.93135\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3798 - accuracy: 0.8404 - val_loss: 0.3983 - val_accuracy: 0.8330\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.93135\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3318 - accuracy: 0.8603 - val_loss: 0.3867 - val_accuracy: 0.8453\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.93135\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3027 - accuracy: 0.8813 - val_loss: 0.4057 - val_accuracy: 0.8238\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.93135\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3100 - accuracy: 0.8731 - val_loss: 0.3742 - val_accuracy: 0.8627\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.93135\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2788 - accuracy: 0.8939 - val_loss: 0.3423 - val_accuracy: 0.8730\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.93135\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2347 - accuracy: 0.9136 - val_loss: 0.3684 - val_accuracy: 0.8648\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.93135\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2400 - accuracy: 0.8938 - val_loss: 0.3846 - val_accuracy: 0.8484\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.93135\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2271 - accuracy: 0.9085 - val_loss: 0.3050 - val_accuracy: 0.9037\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.93135\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1648 - accuracy: 0.9473 - val_loss: 0.2987 - val_accuracy: 0.9232\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.93135\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1590 - accuracy: 0.9488 - val_loss: 0.2971 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.93135\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1326 - accuracy: 0.9594 - val_loss: 0.2936 - val_accuracy: 0.9201\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.93135\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1248 - accuracy: 0.9604 - val_loss: 0.2878 - val_accuracy: 0.9232\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93135\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1087 - accuracy: 0.9661 - val_loss: 0.2756 - val_accuracy: 0.9324\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.93135 to 0.93238, saving model to best_model.h5\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1190 - accuracy: 0.9616 - val_loss: 0.2916 - val_accuracy: 0.9242\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.93238\n",
            "Epoch 31/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0857 - accuracy: 0.9740 - val_loss: 0.2908 - val_accuracy: 0.9273\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.93238\n",
            "Epoch 32/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0910 - accuracy: 0.9695 - val_loss: 0.2826 - val_accuracy: 0.9262\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.93238\n",
            "Epoch 33/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1076 - accuracy: 0.9654 - val_loss: 0.3270 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.93238\n",
            "Epoch 34/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1161 - accuracy: 0.9581 - val_loss: 0.3052 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.93238\n",
            "Epoch 00034: early stopping\n",
            "Score for fold 3: loss of 2.112579584121704; accuracy of 49.743589758872986%\n",
            "-----------------------\n",
            "Training for fold 4 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 9ms/step - loss: 0.6938 - accuracy: 0.5137 - val_loss: 0.6838 - val_accuracy: 0.5533\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.93238\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5686 - val_loss: 0.6746 - val_accuracy: 0.5891\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.93238\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6734 - accuracy: 0.5902 - val_loss: 0.6620 - val_accuracy: 0.5809\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.93238\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5957 - val_loss: 0.6527 - val_accuracy: 0.6189\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93238\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6569 - accuracy: 0.5971 - val_loss: 0.6446 - val_accuracy: 0.6219\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.93238\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6421 - accuracy: 0.6236 - val_loss: 0.6229 - val_accuracy: 0.6588\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93238\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6187 - accuracy: 0.6592 - val_loss: 0.6070 - val_accuracy: 0.6619\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93238\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6046 - accuracy: 0.6674 - val_loss: 0.5933 - val_accuracy: 0.6732\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93238\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5930 - accuracy: 0.6753 - val_loss: 0.5859 - val_accuracy: 0.6619\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.93238\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5757 - accuracy: 0.6994 - val_loss: 0.5475 - val_accuracy: 0.7131\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93238\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5348 - accuracy: 0.7307 - val_loss: 0.5172 - val_accuracy: 0.7531\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93238\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5072 - accuracy: 0.7512 - val_loss: 0.4905 - val_accuracy: 0.7623\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93238\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4996 - accuracy: 0.7328 - val_loss: 0.4886 - val_accuracy: 0.7715\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.93238\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 0.7891 - val_loss: 0.4490 - val_accuracy: 0.7787\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.93238\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4148 - accuracy: 0.8168 - val_loss: 0.4263 - val_accuracy: 0.8023\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.93238\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3945 - accuracy: 0.8283 - val_loss: 0.3978 - val_accuracy: 0.8258\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.93238\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3569 - accuracy: 0.8479 - val_loss: 0.4074 - val_accuracy: 0.8340\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.93238\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 0.8414 - val_loss: 0.3699 - val_accuracy: 0.8432\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.93238\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8737 - val_loss: 0.3433 - val_accuracy: 0.8627\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.93238\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2672 - accuracy: 0.9013 - val_loss: 0.3292 - val_accuracy: 0.8709\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.93238\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2664 - accuracy: 0.8942 - val_loss: 0.3254 - val_accuracy: 0.8863\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.93238\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2313 - accuracy: 0.9145 - val_loss: 0.2846 - val_accuracy: 0.8955\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.93238\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2145 - accuracy: 0.9185 - val_loss: 0.2733 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.93238\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9380 - val_loss: 0.2885 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.93238\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1800 - accuracy: 0.9297 - val_loss: 0.2511 - val_accuracy: 0.9211\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.93238\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1492 - accuracy: 0.9510 - val_loss: 0.2533 - val_accuracy: 0.9252\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.93238\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1375 - accuracy: 0.9531 - val_loss: 0.2662 - val_accuracy: 0.9242\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.93238\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1292 - accuracy: 0.9608 - val_loss: 0.3107 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93238\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1308 - accuracy: 0.9532 - val_loss: 0.2629 - val_accuracy: 0.9293\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.93238\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1195 - accuracy: 0.9563 - val_loss: 0.2743 - val_accuracy: 0.9232\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.93238\n",
            "Epoch 00030: early stopping\n",
            "Score for fold 4: loss of 1.7841007709503174; accuracy of 54.615384340286255%\n",
            "-----------------------\n",
            "Training for fold 5 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 8ms/step - loss: 0.7020 - accuracy: 0.4804 - val_loss: 0.6850 - val_accuracy: 0.5430\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.93238\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6819 - accuracy: 0.5636 - val_loss: 0.6752 - val_accuracy: 0.5768\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.93238\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.5704 - val_loss: 0.6726 - val_accuracy: 0.5676\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.93238\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6701 - accuracy: 0.5795 - val_loss: 0.6593 - val_accuracy: 0.6055\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93238\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6554 - accuracy: 0.5970 - val_loss: 0.6511 - val_accuracy: 0.6045\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.93238\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6403 - accuracy: 0.6255 - val_loss: 0.6411 - val_accuracy: 0.6383\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93238\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6276 - accuracy: 0.6464 - val_loss: 0.6309 - val_accuracy: 0.6414\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93238\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6131 - accuracy: 0.6556 - val_loss: 0.6110 - val_accuracy: 0.6639\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93238\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.5877 - accuracy: 0.6899 - val_loss: 0.5840 - val_accuracy: 0.6895\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.93238\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5567 - accuracy: 0.7134 - val_loss: 0.5893 - val_accuracy: 0.6609\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93238\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5447 - accuracy: 0.7203 - val_loss: 0.5355 - val_accuracy: 0.7367\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93238\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5000 - accuracy: 0.7501 - val_loss: 0.5070 - val_accuracy: 0.7592\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93238\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4812 - accuracy: 0.7671 - val_loss: 0.4923 - val_accuracy: 0.7582\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.93238\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4538 - accuracy: 0.7862 - val_loss: 0.5100 - val_accuracy: 0.7439\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.93238\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.7965 - val_loss: 0.4578 - val_accuracy: 0.7900\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.93238\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4072 - accuracy: 0.8187 - val_loss: 0.4177 - val_accuracy: 0.8207\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.93238\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3563 - accuracy: 0.8424 - val_loss: 0.4268 - val_accuracy: 0.8176\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.93238\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3289 - accuracy: 0.8613 - val_loss: 0.3890 - val_accuracy: 0.8422\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.93238\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3029 - accuracy: 0.8720 - val_loss: 0.3753 - val_accuracy: 0.8607\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.93238\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2598 - accuracy: 0.9020 - val_loss: 0.3455 - val_accuracy: 0.8576\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.93238\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.9025 - val_loss: 0.3461 - val_accuracy: 0.8781\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.93238\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2142 - accuracy: 0.9151 - val_loss: 0.3264 - val_accuracy: 0.8914\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.93238\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1930 - accuracy: 0.9230 - val_loss: 0.3027 - val_accuracy: 0.9016\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.93238\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1607 - accuracy: 0.9384 - val_loss: 0.3079 - val_accuracy: 0.9088\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.93238\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1703 - accuracy: 0.9371 - val_loss: 0.2922 - val_accuracy: 0.9129\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.93238\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1338 - accuracy: 0.9525 - val_loss: 0.3091 - val_accuracy: 0.9262\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.93238\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1296 - accuracy: 0.9561 - val_loss: 0.3008 - val_accuracy: 0.9221\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.93238\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1318 - accuracy: 0.9549 - val_loss: 0.3301 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93238\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1678 - accuracy: 0.9320 - val_loss: 0.3326 - val_accuracy: 0.9109\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.93238\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1220 - accuracy: 0.9538 - val_loss: 0.3215 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.93238\n",
            "Epoch 00030: early stopping\n",
            "Score for fold 5: loss of 1.9597108364105225; accuracy of 50.76923370361328%\n",
            "-----------------------\n",
            "Training for fold 6 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 8ms/step - loss: 0.6942 - accuracy: 0.5146 - val_loss: 0.6851 - val_accuracy: 0.5779\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.93238\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6850 - accuracy: 0.5637 - val_loss: 0.6758 - val_accuracy: 0.5809\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.93238\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6748 - accuracy: 0.5885 - val_loss: 0.6671 - val_accuracy: 0.5943\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.93238\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6669 - accuracy: 0.6012 - val_loss: 0.6561 - val_accuracy: 0.6055\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93238\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6555 - accuracy: 0.6123 - val_loss: 0.6503 - val_accuracy: 0.6158\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.93238\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6453 - accuracy: 0.6185 - val_loss: 0.6346 - val_accuracy: 0.6393\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93238\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6190 - accuracy: 0.6663 - val_loss: 0.6260 - val_accuracy: 0.6363\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93238\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6080 - accuracy: 0.6766 - val_loss: 0.6153 - val_accuracy: 0.6516\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93238\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 0.6031 - accuracy: 0.6676 - val_loss: 0.5887 - val_accuracy: 0.6773\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.93238\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5624 - accuracy: 0.7074 - val_loss: 0.5489 - val_accuracy: 0.7346\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93238\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.7315 - val_loss: 0.5202 - val_accuracy: 0.7418\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93238\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5135 - accuracy: 0.7414 - val_loss: 0.4972 - val_accuracy: 0.7551\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93238\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4649 - accuracy: 0.7869 - val_loss: 0.4757 - val_accuracy: 0.7807\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.93238\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4497 - accuracy: 0.7861 - val_loss: 0.4519 - val_accuracy: 0.7900\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.93238\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.3980 - accuracy: 0.8206 - val_loss: 0.4356 - val_accuracy: 0.7889\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.93238\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3883 - accuracy: 0.8260 - val_loss: 0.4009 - val_accuracy: 0.8197\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.93238\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3395 - accuracy: 0.8616 - val_loss: 0.3800 - val_accuracy: 0.8391\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.93238\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3070 - accuracy: 0.8729 - val_loss: 0.3529 - val_accuracy: 0.8535\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.93238\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3012 - accuracy: 0.8724 - val_loss: 0.3406 - val_accuracy: 0.8607\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.93238\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2476 - accuracy: 0.9070 - val_loss: 0.3206 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.93238\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2344 - accuracy: 0.9119 - val_loss: 0.3004 - val_accuracy: 0.8904\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.93238\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2231 - accuracy: 0.9090 - val_loss: 0.3006 - val_accuracy: 0.8873\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.93238\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1809 - accuracy: 0.9356 - val_loss: 0.2839 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.93238\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1684 - accuracy: 0.9438 - val_loss: 0.2948 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.93238\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1554 - accuracy: 0.9428 - val_loss: 0.3071 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.93238\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1415 - accuracy: 0.9443 - val_loss: 0.2783 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.93238\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9586 - val_loss: 0.2930 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.93238\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1442 - accuracy: 0.9411 - val_loss: 0.2934 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93238\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1197 - accuracy: 0.9563 - val_loss: 0.2781 - val_accuracy: 0.9201\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.93238\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1043 - accuracy: 0.9652 - val_loss: 0.3596 - val_accuracy: 0.8791\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.93238\n",
            "Epoch 31/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9358 - val_loss: 0.4800 - val_accuracy: 0.8678\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.93238\n",
            "Epoch 32/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1646 - accuracy: 0.9341 - val_loss: 0.3078 - val_accuracy: 0.9211\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.93238\n",
            "Epoch 33/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1006 - accuracy: 0.9671 - val_loss: 0.2955 - val_accuracy: 0.9314\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.93238\n",
            "Epoch 34/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9706 - val_loss: 0.2959 - val_accuracy: 0.9211\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.93238\n",
            "Epoch 00034: early stopping\n",
            "Score for fold 6: loss of 2.2210614681243896; accuracy of 51.79487466812134%\n",
            "-----------------------\n",
            "Training for fold 7 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 8ms/step - loss: 0.7029 - accuracy: 0.4972 - val_loss: 0.6799 - val_accuracy: 0.5707\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.93238\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6785 - accuracy: 0.5630 - val_loss: 0.6711 - val_accuracy: 0.5840\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.93238\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5956 - val_loss: 0.6570 - val_accuracy: 0.5963\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.93238\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6496 - accuracy: 0.6141 - val_loss: 0.6476 - val_accuracy: 0.6240\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93238\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6437 - accuracy: 0.6269 - val_loss: 0.6368 - val_accuracy: 0.6301\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.93238\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6238 - accuracy: 0.6561 - val_loss: 0.6134 - val_accuracy: 0.6516\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93238\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6029 - accuracy: 0.6867 - val_loss: 0.6164 - val_accuracy: 0.6557\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93238\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5863 - accuracy: 0.6880 - val_loss: 0.5825 - val_accuracy: 0.6926\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93238\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.5678 - accuracy: 0.7099 - val_loss: 0.5750 - val_accuracy: 0.6926\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.93238\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5390 - accuracy: 0.7190 - val_loss: 0.5341 - val_accuracy: 0.7264\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93238\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4979 - accuracy: 0.7618 - val_loss: 0.5374 - val_accuracy: 0.7326\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93238\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4859 - accuracy: 0.7596 - val_loss: 0.4764 - val_accuracy: 0.7828\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93238\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.8017 - val_loss: 0.4595 - val_accuracy: 0.7797\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.93238\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4190 - accuracy: 0.8159 - val_loss: 0.4419 - val_accuracy: 0.8033\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.93238\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4037 - accuracy: 0.8163 - val_loss: 0.4343 - val_accuracy: 0.7951\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.93238\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3736 - accuracy: 0.8320 - val_loss: 0.4113 - val_accuracy: 0.8227\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.93238\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3256 - accuracy: 0.8725 - val_loss: 0.3836 - val_accuracy: 0.8412\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.93238\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2965 - accuracy: 0.8838 - val_loss: 0.3686 - val_accuracy: 0.8545\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.93238\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2752 - accuracy: 0.8965 - val_loss: 0.3653 - val_accuracy: 0.8484\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.93238\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2410 - accuracy: 0.9086 - val_loss: 0.3272 - val_accuracy: 0.8883\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.93238\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2278 - accuracy: 0.9123 - val_loss: 0.3110 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.93238\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1856 - accuracy: 0.9364 - val_loss: 0.2992 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.93238\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1698 - accuracy: 0.9461 - val_loss: 0.2967 - val_accuracy: 0.8924\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.93238\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1675 - accuracy: 0.9372 - val_loss: 0.3108 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.93238\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1473 - accuracy: 0.9489 - val_loss: 0.2788 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.93238\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9524 - val_loss: 0.3290 - val_accuracy: 0.8781\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.93238\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1569 - accuracy: 0.9377 - val_loss: 0.3556 - val_accuracy: 0.8873\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.93238\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1486 - accuracy: 0.9484 - val_loss: 0.2926 - val_accuracy: 0.9242\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93238\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1159 - accuracy: 0.9557 - val_loss: 0.2972 - val_accuracy: 0.9201\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.93238\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1065 - accuracy: 0.9670 - val_loss: 0.2989 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.93238\n",
            "Epoch 00030: early stopping\n",
            "Score for fold 7: loss of 2.1685173511505127; accuracy of 46.92307710647583%\n",
            "-----------------------\n",
            "Training for fold 8 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 8ms/step - loss: 0.6924 - accuracy: 0.5093 - val_loss: 0.6797 - val_accuracy: 0.5666\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.93238\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6769 - accuracy: 0.5733 - val_loss: 0.6713 - val_accuracy: 0.5707\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.93238\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6741 - accuracy: 0.5694 - val_loss: 0.6652 - val_accuracy: 0.5891\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.93238\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6637 - accuracy: 0.5938 - val_loss: 0.6573 - val_accuracy: 0.6117\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.93238\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6521 - accuracy: 0.6176 - val_loss: 0.6462 - val_accuracy: 0.6240\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.93238\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6449 - accuracy: 0.6324 - val_loss: 0.6296 - val_accuracy: 0.6516\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.93238\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6206 - accuracy: 0.6626 - val_loss: 0.6107 - val_accuracy: 0.6762\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.93238\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6071 - accuracy: 0.6782 - val_loss: 0.6199 - val_accuracy: 0.6496\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.93238\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5964 - accuracy: 0.6793 - val_loss: 0.5877 - val_accuracy: 0.6680\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.93238\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5673 - accuracy: 0.7075 - val_loss: 0.5696 - val_accuracy: 0.6988\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.93238\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5406 - accuracy: 0.7231 - val_loss: 0.5409 - val_accuracy: 0.7264\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.93238\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5257 - accuracy: 0.7256 - val_loss: 0.5342 - val_accuracy: 0.7162\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.93238\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4862 - accuracy: 0.7677 - val_loss: 0.4956 - val_accuracy: 0.7572\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.93238\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4529 - accuracy: 0.8031 - val_loss: 0.4549 - val_accuracy: 0.7807\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.93238\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4220 - accuracy: 0.8067 - val_loss: 0.4250 - val_accuracy: 0.8105\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.93238\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3700 - accuracy: 0.8396 - val_loss: 0.4038 - val_accuracy: 0.8197\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.93238\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3484 - accuracy: 0.8545 - val_loss: 0.3803 - val_accuracy: 0.8361\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.93238\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3194 - accuracy: 0.8628 - val_loss: 0.3577 - val_accuracy: 0.8555\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.93238\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2983 - accuracy: 0.8737 - val_loss: 0.3408 - val_accuracy: 0.8678\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.93238\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2606 - accuracy: 0.8956 - val_loss: 0.3055 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.93238\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2422 - accuracy: 0.9006 - val_loss: 0.3127 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.93238\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2113 - accuracy: 0.9233 - val_loss: 0.2851 - val_accuracy: 0.8986\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.93238\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1860 - accuracy: 0.9265 - val_loss: 0.2995 - val_accuracy: 0.8996\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.93238\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1755 - accuracy: 0.9374 - val_loss: 0.3129 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.93238\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1516 - accuracy: 0.9486 - val_loss: 0.3313 - val_accuracy: 0.8852\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.93238\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9442 - val_loss: 0.2677 - val_accuracy: 0.9221\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.93238\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9379 - val_loss: 0.2671 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.93238\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1438 - accuracy: 0.9482 - val_loss: 0.2697 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.93238\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1194 - accuracy: 0.9603 - val_loss: 0.2543 - val_accuracy: 0.9334\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.93238 to 0.93340, saving model to best_model.h5\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.9678 - val_loss: 0.2525 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.93340 to 0.93648, saving model to best_model.h5\n",
            "Epoch 31/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0812 - accuracy: 0.9753 - val_loss: 0.2893 - val_accuracy: 0.9170\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.93648\n",
            "Epoch 32/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9710 - val_loss: 0.2698 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.93648 to 0.93750, saving model to best_model.h5\n",
            "Epoch 33/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0601 - accuracy: 0.9830 - val_loss: 0.2987 - val_accuracy: 0.9314\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.93750\n",
            "Epoch 34/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9797 - val_loss: 0.2819 - val_accuracy: 0.9365\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.93750\n",
            "Epoch 35/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0510 - accuracy: 0.9845 - val_loss: 0.2713 - val_accuracy: 0.9498\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.93750 to 0.94980, saving model to best_model.h5\n",
            "Epoch 00035: early stopping\n",
            "Score for fold 8: loss of 2.616335868835449; accuracy of 51.79487466812134%\n",
            "-----------------------\n",
            "Training for fold 9 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 9ms/step - loss: 0.6952 - accuracy: 0.5105 - val_loss: 0.6852 - val_accuracy: 0.5318\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.94980\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.5491 - val_loss: 0.6764 - val_accuracy: 0.5461\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.94980\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6784 - accuracy: 0.5656 - val_loss: 0.6704 - val_accuracy: 0.5676\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.94980\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6653 - accuracy: 0.5928 - val_loss: 0.6561 - val_accuracy: 0.6066\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.94980\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.6540 - accuracy: 0.6117 - val_loss: 0.6525 - val_accuracy: 0.6025\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.94980\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6393 - accuracy: 0.6282 - val_loss: 0.6376 - val_accuracy: 0.6434\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.94980\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6202 - accuracy: 0.6645 - val_loss: 0.6241 - val_accuracy: 0.6291\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.94980\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6059 - accuracy: 0.6730 - val_loss: 0.6067 - val_accuracy: 0.6568\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.94980\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5794 - accuracy: 0.7034 - val_loss: 0.5882 - val_accuracy: 0.6865\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.94980\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5610 - accuracy: 0.7081 - val_loss: 0.5790 - val_accuracy: 0.7008\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.94980\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5567 - accuracy: 0.7091 - val_loss: 0.5623 - val_accuracy: 0.7264\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.94980\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5261 - accuracy: 0.7389 - val_loss: 0.5161 - val_accuracy: 0.7633\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.94980\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4861 - accuracy: 0.7813 - val_loss: 0.5192 - val_accuracy: 0.7592\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.94980\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4669 - accuracy: 0.7767 - val_loss: 0.4829 - val_accuracy: 0.7736\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.94980\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4259 - accuracy: 0.8078 - val_loss: 0.4630 - val_accuracy: 0.8033\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.94980\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4042 - accuracy: 0.8228 - val_loss: 0.4523 - val_accuracy: 0.7869\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.94980\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3711 - accuracy: 0.8385 - val_loss: 0.4322 - val_accuracy: 0.8094\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.94980\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3515 - accuracy: 0.8434 - val_loss: 0.4806 - val_accuracy: 0.7643\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.94980\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3371 - accuracy: 0.8437 - val_loss: 0.3567 - val_accuracy: 0.8576\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.94980\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2656 - accuracy: 0.9012 - val_loss: 0.3805 - val_accuracy: 0.8494\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.94980\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2652 - accuracy: 0.8938 - val_loss: 0.3302 - val_accuracy: 0.8791\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.94980\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2424 - accuracy: 0.9046 - val_loss: 0.3511 - val_accuracy: 0.8781\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.94980\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2239 - accuracy: 0.9161 - val_loss: 0.3348 - val_accuracy: 0.8750\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.94980\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.2098 - accuracy: 0.9204 - val_loss: 0.3334 - val_accuracy: 0.8689\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.94980\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1944 - accuracy: 0.9240 - val_loss: 0.2934 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.94980\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1707 - accuracy: 0.9401 - val_loss: 0.3296 - val_accuracy: 0.8832\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.94980\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1681 - accuracy: 0.9374 - val_loss: 0.2861 - val_accuracy: 0.9119\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.94980\n",
            "Epoch 28/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.1321 - accuracy: 0.9576 - val_loss: 0.2652 - val_accuracy: 0.9180\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.94980\n",
            "Epoch 29/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1245 - accuracy: 0.9562 - val_loss: 0.2655 - val_accuracy: 0.9191\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.94980\n",
            "Epoch 30/50\n",
            "28/28 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9700 - val_loss: 0.2521 - val_accuracy: 0.9303\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.94980\n",
            "Epoch 31/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0829 - accuracy: 0.9765 - val_loss: 0.2561 - val_accuracy: 0.9314\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.94980\n",
            "Epoch 32/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9767 - val_loss: 0.2960 - val_accuracy: 0.9242\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.94980\n",
            "Epoch 33/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0799 - accuracy: 0.9787 - val_loss: 0.2632 - val_accuracy: 0.9262\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.94980\n",
            "Epoch 34/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0667 - accuracy: 0.9805 - val_loss: 0.2853 - val_accuracy: 0.9150\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.94980\n",
            "Epoch 35/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.0658 - accuracy: 0.9845 - val_loss: 0.2855 - val_accuracy: 0.9334\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.94980\n",
            "Epoch 00035: early stopping\n",
            "Score for fold 9: loss of 1.9612921476364136; accuracy of 54.10256385803223%\n",
            "-----------------------\n",
            "Training for fold 10 ...\n",
            "Epoch 1/50\n",
            "28/28 [==============================] - 1s 8ms/step - loss: 0.6986 - accuracy: 0.5076 - val_loss: 0.6849 - val_accuracy: 0.5584\n",
            "\n",
            "Epoch 00001: val_accuracy did not improve from 0.94980\n",
            "Epoch 2/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.5775 - val_loss: 0.6750 - val_accuracy: 0.5707\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.94980\n",
            "Epoch 3/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6755 - accuracy: 0.5658 - val_loss: 0.6605 - val_accuracy: 0.5922\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.94980\n",
            "Epoch 4/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.5943 - val_loss: 0.6468 - val_accuracy: 0.6250\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.94980\n",
            "Epoch 5/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6395 - accuracy: 0.6215 - val_loss: 0.6379 - val_accuracy: 0.6352\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.94980\n",
            "Epoch 6/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6332 - accuracy: 0.6397 - val_loss: 0.6207 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.94980\n",
            "Epoch 7/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.6061 - accuracy: 0.6678 - val_loss: 0.6068 - val_accuracy: 0.6701\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.94980\n",
            "Epoch 8/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5954 - accuracy: 0.6791 - val_loss: 0.5793 - val_accuracy: 0.6967\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.94980\n",
            "Epoch 9/50\n",
            "28/28 [==============================] - 0s 3ms/step - loss: 0.5651 - accuracy: 0.7205 - val_loss: 0.5605 - val_accuracy: 0.7428\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.94980\n",
            "Epoch 10/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5514 - accuracy: 0.7186 - val_loss: 0.5479 - val_accuracy: 0.7172\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.94980\n",
            "Epoch 11/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.5259 - accuracy: 0.7341 - val_loss: 0.5122 - val_accuracy: 0.7418\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.94980\n",
            "Epoch 12/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4824 - accuracy: 0.7746 - val_loss: 0.4897 - val_accuracy: 0.7797\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.94980\n",
            "Epoch 13/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4431 - accuracy: 0.8080 - val_loss: 0.4532 - val_accuracy: 0.7859\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.94980\n",
            "Epoch 14/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.4095 - accuracy: 0.8177 - val_loss: 0.4222 - val_accuracy: 0.8258\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.94980\n",
            "Epoch 15/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3810 - accuracy: 0.8380 - val_loss: 0.4377 - val_accuracy: 0.8125\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.94980\n",
            "Epoch 16/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3544 - accuracy: 0.8537 - val_loss: 0.3881 - val_accuracy: 0.8432\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.94980\n",
            "Epoch 17/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3053 - accuracy: 0.8850 - val_loss: 0.3957 - val_accuracy: 0.8504\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.94980\n",
            "Epoch 18/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.3046 - accuracy: 0.8776 - val_loss: 0.3468 - val_accuracy: 0.8791\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.94980\n",
            "Epoch 19/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2612 - accuracy: 0.8964 - val_loss: 0.3233 - val_accuracy: 0.8760\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.94980\n",
            "Epoch 20/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2401 - accuracy: 0.9075 - val_loss: 0.2967 - val_accuracy: 0.9047\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.94980\n",
            "Epoch 21/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.2010 - accuracy: 0.9380 - val_loss: 0.3121 - val_accuracy: 0.8945\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.94980\n",
            "Epoch 22/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1760 - accuracy: 0.9400 - val_loss: 0.2841 - val_accuracy: 0.9139\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.94980\n",
            "Epoch 23/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1616 - accuracy: 0.9474 - val_loss: 0.3100 - val_accuracy: 0.9078\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.94980\n",
            "Epoch 24/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9538 - val_loss: 0.3292 - val_accuracy: 0.8873\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.94980\n",
            "Epoch 25/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1587 - accuracy: 0.9447 - val_loss: 0.3126 - val_accuracy: 0.9119\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.94980\n",
            "Epoch 26/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1548 - accuracy: 0.9499 - val_loss: 0.3462 - val_accuracy: 0.9057\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.94980\n",
            "Epoch 27/50\n",
            "28/28 [==============================] - 0s 4ms/step - loss: 0.1460 - accuracy: 0.9423 - val_loss: 0.2961 - val_accuracy: 0.9221\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.94980\n",
            "Epoch 00027: early stopping\n",
            "Score for fold 10: loss of 1.7694958448410034; accuracy of 51.02564096450806%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcflbjr9qH3b",
        "outputId": "85d43784-3bdc-4d4f-bde8-c5f66ac299ec"
      },
      "source": [
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 1.8731220960617065 - Accuracy: 48.33759665489197%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 1.7169853448867798 - Accuracy: 49.87212419509888%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 2.112579584121704 - Accuracy: 49.743589758872986%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 1.7841007709503174 - Accuracy: 54.615384340286255%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.9597108364105225 - Accuracy: 50.76923370361328%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 2.2210614681243896 - Accuracy: 51.79487466812134%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 2.1685173511505127 - Accuracy: 46.92307710647583%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 2.616335868835449 - Accuracy: 51.79487466812134%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 1.9612921476364136 - Accuracy: 54.10256385803223%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 1.7694958448410034 - Accuracy: 51.02564096450806%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 50.897895991802216 (+- 2.246565790979188)\n",
            "> Loss: 2.0183201313018797\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXm59rRlTnrI"
      },
      "source": [
        "## Evaluating ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc_RtIYKTnrI",
        "outputId": "c9403e21-bd77-43a7-b4d2-51f017f83f08"
      },
      "source": [
        "score = model.evaluate(X_test, \n",
        "                       y_test, \n",
        "                      batch_size=20,\n",
        "                      steps=3)\n",
        "print(X_test)\n",
        "X_real = [[-530.6179199,\t18.38093376,\t-13.43567276,\t13.50225735,\t-8.69652462,\t5.986907959,\t-9.717094421,\t1.417438745,\t-4.564946175,\t1.673386455,\t0.156302944,\t-4.657121658,\t3.823266745,\t-1.463427186,\t0.626839399,\t-5.152077675,\t-2.040416241,\t-2.548900604,\t-0.977410972,\t-0.468565971]]\n",
        "y_real = model.predict_classes(X_real)\n",
        "\n",
        "\n",
        "print(y_real)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 3ms/step - loss: 0.3809 - accuracy: 0.9167\n",
            "[[-4.01977004e-01  3.37002539e-01 -3.53674419e-01 ...  4.03372090e-01\n",
            "   7.76395598e-01 -5.79829608e-01]\n",
            " [ 7.19145505e-01  7.94854491e-01 -1.75865132e+00 ... -1.55311902e+00\n",
            "  -6.52866138e-01  1.08455498e+00]\n",
            " [ 1.28373798e+00 -6.38731627e-01 -2.52371083e+00 ... -2.91429559e-01\n",
            "   1.34884591e+00  1.34383564e+00]\n",
            " ...\n",
            " [ 3.06803698e-01 -3.36883143e-01  1.05487327e-01 ... -3.78207755e-01\n",
            "   2.00105438e-01  3.94094231e-01]\n",
            " [-2.93377138e-01 -8.12752515e-01 -1.86072154e-01 ...  1.09984790e+00\n",
            "   4.13213341e-01  6.72868377e-01]\n",
            " [-7.89718320e-01 -9.31502341e-01  1.95229878e-03 ... -5.90833108e-01\n",
            "   8.12945653e-01 -3.51428078e-01]]\n",
            "[1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh2WPIy2TnrI",
        "outputId": "32ef0c0c-23d1-4d56-c2e5-cebaf32a1db7"
      },
      "source": [
        "y_pred = model.predict_classes(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "def plot_confusion_matrix(cm):\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    classNames = ['Negative','Positive']\n",
        "    plt.title('COVID-19 Confusion Matrix')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    tick_marks = np.arange(len(classNames))\n",
        "    plt.xticks(tick_marks, classNames, rotation=45)\n",
        "    plt.yticks(tick_marks, classNames)\n",
        "    s = [['TN','FP'], ['FN', 'TP']]\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
        "    plt.savefig(\"img/Fig6_Confusion_Matrix_50_epochs.png\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dph9ORcWTnrJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db30a46e-3045-4521-e4fc-8c78ed7e6101"
      },
      "source": [
        "def plot_loss_accuracy(clf):\n",
        "    loss_clf = clf.history['loss']\n",
        "    acc_clf = clf.history['accuracy']\n",
        "    epoch_range = list(range(1, 28))\n",
        " \n",
        "    plt.figure()\n",
        "    plt.plot(epoch_range, loss_clf)\n",
        "    plt.title(f\"Loss for {len(epoch_range)} Epochs\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig(\"Fig4_Loss_Per_50_Epochs.png\")\n",
        " \n",
        " \n",
        "    plt.figure()\n",
        "    plt.plot(epoch_range, acc_clf)\n",
        "    plt.title(f\"Accuracy {len(epoch_range)} Epochs\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.savefig(\"Fig5_Accuracy_per_50_epochs.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    y_pred = model.predict_classes(X_test)\n",
        "    print(\"\\n **Confusion Matrix**\\n\")\n",
        "    #print(confusion_matrix(y_test, y_pred))\n",
        "    y_pred = model.predict_classes(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    classNames = ['Negative','Positive']\n",
        "    plt.title('COVID-19 Confusion Matrix')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    tick_marks = np.arange(len(classNames))\n",
        "    plt.xticks(tick_marks, classNames, rotation=45)\n",
        "    plt.yticks(tick_marks, classNames)\n",
        "    s = [['TN','FP'], ['FN', 'TP']]\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
        "    plt.savefig(\"Fig6_Confusion_Matrix_50_epochs.png\")\n",
        "    plt.show()\n",
        "    print(\"\\n **Classification Report**\\n\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        " \n",
        "plot_loss_accuracy(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " **Confusion Matrix**\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " **Classification Report**\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.92       507\n",
            "           1       0.89      0.95      0.92       469\n",
            "\n",
            "    accuracy                           0.92       976\n",
            "   macro avg       0.92      0.92      0.92       976\n",
            "weighted avg       0.92      0.92      0.92       976\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCoEHpnfTnrJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a679967-17c9-44b1-99b3-28f9c8699b26"
      },
      "source": [
        "model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('covid.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp6mtvocl8/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp6mtvocl8/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Y1P0ic2d0P28",
        "outputId": "9c49a563-46cb-4f43-a0c4-c77e1843a010"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('covid.tflite')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5b78a135-fb4a-4dc6-92a0-3e321f180005\", \"covid.tflite\", 191480)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}